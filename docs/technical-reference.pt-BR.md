<div align="center">

# Refer√™ncia T√©cnica

[![English](https://img.shields.io/badge/English-üá¨üáß-blue?style=for-the-badge)](technical-reference.md) [![Portugu√™s (BR)](https://img.shields.io/badge/Portugu√™s-üáßüá∑-green?style=for-the-badge)](technical-reference.pt-BR.md)

</div>

Deep-dive nos detalhes de implementa√ß√£o do Theological LangGraph Agent. Para a vis√£o geral, veja o [README principal](../README.pt-BR.md).

---

## √çndice

- [Implementa√ß√£o LangGraph](#implementa√ß√£o-langgraph)
- [Estrat√©gia de Modelos](#estrat√©gia-de-modelos)
- [Gerenciamento de Prompts e Resili√™ncia](#gerenciamento-de-prompts-e-resili√™ncia)
- [Camada de Governan√ßa](#camada-de-governan√ßa)
- [Fluxo HITL](#fluxo-hitl)
- [Schema do Banco de Dados](#schema-do-banco-de-dados)
- [Servi√ßo de Cache](#servi√ßo-de-cache)
- [Refer√™ncia da API](#refer√™ncia-da-api)
- [Deploy](#deploy)

---

## Implementa√ß√£o LangGraph

### Padr√£o Scatter-Gather

O sistema usa a API `Send` do LangGraph para despachar agentes em paralelo baseado nos m√≥dulos de an√°lise selecionados:

```python
def router_function(state: TheologicalState):
    sends = [Send("intertextual_agent", state)]  # Sempre executa

    if "panorama" in state["selected_modules"]:
        sends.append(Send("panorama_agent", state))

    if "exegese" in state["selected_modules"]:
        sends.append(Send("lexical_agent", state))

    if "historical" in state["selected_modules"]:
        sends.append(Send("historical_agent", state))

    return sends  # Todos os agentes executam em paralelo
```

### Gerenciamento de Estado

Estado tipado usando `TypedDict` com campos de governan√ßa:

```python
class TheologicalState(TypedDict):
    # Entrada
    bible_book: str
    chapter: int
    verses: List[str]
    selected_modules: List[str]

    # Sa√≠das dos agentes
    panorama_content: Optional[str]
    lexical_content: Optional[str]
    historical_content: Optional[str]
    intertextual_content: Optional[str]
    validation_content: Optional[str]
    final_analysis: Optional[str]

    # Governan√ßa
    run_id: Optional[str]
    created_at: Optional[str]
    model_versions: Optional[dict]       # {"panorama_agent": "gemini-2.5-flash", ...}
    tokens_consumed: Optional[dict]      # {"panorama_agent": {"input": N, "output": M}, ...}
    reasoning_steps: Optional[list]      # Trilha de metadados a custo zero
    risk_level: Optional[str]            # "low" | "medium" | "high"
    hitl_status: Optional[str]           # None | "pending" | "approved" | "edited"
```

### Padr√£o DRY dos N√≥s ‚Äî `_build_node_result()`

Todos os n√≥s de an√°lise compartilham a mesma l√≥gica de retorno de governan√ßa. Em vez de repetir em cada n√≥, usamos um √∫nico helper:

```python
def _build_node_result(
    state, node_name, model_name, response, start_time,
    output_field, extra_fields=None, extra_reasoning=None,
) -> dict:
    # 1. Calcula dura√ß√£o + extrai uso de tokens
    # 2. Log estruturado com correla√ß√£o run_id
    # 3. Merge de model_versions, tokens_consumed, reasoning_steps (imut√°vel)
    # 4. Sanitiza output do LLM
    # 5. Retorna dict completo de governan√ßa
```

**Antes:** Cada n√≥ tinha ~48 linhas. **Depois:** Cada n√≥ tem ~20 linhas.

Exemplo de n√≥:
```python
def panorama_node(state: TheologicalState):
    start = time.time()
    model = get_panorama_model()
    # ... constr√≥i prompt + mensagens ...
    response = model.invoke(messages)
    return _build_node_result(
        state, "panorama_agent", ModelTier.FLASH, response, start,
        output_field="panorama_content",
        raw_response=response,
    )
```

---

## Estrat√©gia de Modelos

### Arquitetura em 3 Camadas

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Atribui√ß√£o de Modelos                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ LITE             ‚îÇ FLASH                ‚îÇ TOP                 ‚îÇ
‚îÇ flash-lite       ‚îÇ 2.5-flash            ‚îÇ 3-flash-preview     ‚îÇ
‚îÇ 10 RPM           ‚îÇ 5 RPM                ‚îÇ 5 RPM               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Intertextual     ‚îÇ Panorama             ‚îÇ Validador           ‚îÇ
‚îÇ                  ‚îÇ L√©xico               ‚îÇ Sintetizador        ‚îÇ
‚îÇ                  ‚îÇ Hist√≥rico            ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Cadeia de Fallback

```
gemini-3-flash-preview ‚Üí gemini-2.5-flash ‚Üí gemini-2.5-flash-lite ‚Üí gemini-2.0-flash-lite
```

Se o modelo prim√°rio retorna 429 (rate limited) ou est√° depreciado, o cliente migra automaticamente para o pr√≥ximo tier. O modelo efetivamente utilizado √© registrado em `model_versions`.

### Configura√ß√µes de Temperatura

| N√≥ | Temperatura | Justificativa |
|----|-------------|---------------|
| L√©xico | 0.1 | Precis√£o m√°xima para estudo de palavras |
| Panorama | 0.2 | Equil√≠brio entre acur√°cia e contexto |
| Hist√≥rico | 0.2 | Equil√≠brio entre acur√°cia e contexto |
| Intertextual | 0.2 | Equil√≠brio entre acur√°cia e contexto |
| Validador | 0.1 | Avalia√ß√£o de risco consistente |
| Sintetizador | 0.4 | S√≠ntese criativa com calor pastoral |

---

## Gerenciamento de Prompts e Resili√™ncia

A engenharia de prompts √© desacoplada da l√≥gica core via **LangSmith Prompt Hub**, garantindo agilidade sem necessidade de redeploy.

### Modelo de Execu√ß√£o H√≠brido

1. **Prim√°rio (Hub):** O n√≥ tenta baixar o prompt publicado mais recente do LangSmith.
2. **Fallback (JSON Local):** Se a chamada ao Hub falhar (rede, 429, erros de chave de API), o sistema degrada automaticamente para uma r√©plica JSON local em `src/app/utils/fallbacks/`.
3. **Utilit√°rio de Resili√™ncia:** O utilit√°rio `hub_fallback.py` gerencia essa transi√ß√£o, garantindo que a `GOOGLE_API_KEY` obrigat√≥ria seja preservada mesmo quando o `secrets_from_env` do LangSmith falha.

### Rastreamento de Vers√£o

Cada execu√ß√£o de prompt captura o **Hash de Commit do Prompt** para garantir auditabilidade total:
- **Modo Hub:** Extrai o `lc_hub_commit_hash` dos metadados do LangSmith.
- **Modo Fallback:** Usa o hash armazenado no `prompts_fallback.json` durante a √∫ltima sincroniza√ß√£o.
- **Propaga√ß√£o:** O hash √© injetado nos `reasoning_steps` e nos logs estruturados.

### Script de Sincroniza√ß√£o

Um script utilit√°rio `sync_prompts.py` √© usado para atualizar o cache de fallback local:
- Extrai templates de mensagens e configura√ß√µes de modelo (temperatura/model_name).
- Captura e armazena o hash de commit espec√≠fico.
- **Frequ√™ncia:** Altamente recomendado executar este script durante o CI/CD para garantir que o JSON local reflita os prompts mais recentes aprovados em produ√ß√£o no Hub.


---

## Camada de Governan√ßa

### Rastreamento de Tokens

Extra√≠do do `usage_metadata` em cada resposta do LLM (custo zero adicional):

```python
def extract_token_usage(response) -> dict:
    if hasattr(response, "usage_metadata") and response.usage_metadata:
        return {
            "input": response.usage_metadata.input_tokens,
            "output": response.usage_metadata.output_tokens,
        }
    return {}
```

### Trilha de Racioc√≠nio (Reasoning Steps)

Cada n√≥ adiciona um passo de racioc√≠nio ao estado:

```json
[
  {"node": "panorama_agent", "model": "gemini-2.5-flash", "tokens": {"input": 1200, "output": 3400}, "duration_ms": 8500},
  {"node": "lexical_agent", "model": "gemini-2.5-flash", "tokens": {"input": 980, "output": 4100}, "duration_ms": 12300},
  {"node": "theological_validator", "model": "gemini-3-flash-preview", "risk_level": "low", "alerts": [], "duration_ms": 6200}
]
```

### Logging Estruturado

Logs formatados em JSON com correla√ß√£o `run_id` para rastreamento entre n√≥s:

```json
{
  "timestamp": "2026-02-15T04:13:01Z",
  "level": "INFO",
  "logger": "app.agent.build",
  "message": "panorama_agent completed",
  "event": "node_complete",
  "node": "panorama_agent",
  "model": "gemini-2.5-flash",
  "prompt_commit_hash": "a7b2c...",
  "tokens": {"input": 1200, "output": 3400},
  "duration_ms": 8500,
  "run_id": "a1b2c3d4"
}
```

> **Exemplo Real:** Veja [`samples/`](../samples/) para logs de entrada/sa√≠da reais mostrando uso de tokens e passos de racioc√≠nio.
```

### Servi√ßo de Auditoria

Todo run de an√°lise √© persistido na tabela `analysis_runs`:
- Sucesso: metadados completos (tokens, modelos, dura√ß√£o, n√≠vel de risco)
- Falha: mensagem de erro, traceback, metadados parciais
- Usa UPSERT para idempot√™ncia (mesmo `run_id` ‚Üí atualiza, n√£o duplica)

---

## Fluxo HITL

### Ciclo de Vida

```mermaid
sequenceDiagram
    participant U as Usu√°rio
    participant API as FastAPI
    participant G as LangGraph
    participant V as Validador
    participant DB as Supabase
    participant E as Email

    U->>API: POST /analyze
    API->>G: Executa grafo
    G->>V: Valida an√°lise
    V-->>G: risk_level = "high"
    G->>DB: Persiste estado completo (hitl_reviews)
    G->>E: Envia email de notifica√ß√£o
    G-->>API: hitl_status = "pending"
    API-->>U: 200 + run_id + hitl_status

    Note over U: Revisor recebe email

    U->>API: POST /hitl/{run_id}/approve
    API->>DB: Carrega estado salvo
    API->>G: Executa apenas sintetizador
    G-->>API: final_analysis
    API->>DB: Atualiza status = "approved"
    API-->>U: 200 + an√°lise final
```

### Endpoint de Aprova√ß√£o

O endpoint de aprova√ß√£o reconstr√≥i o estado do agente a partir do banco de dados e executa **apenas o sintetizador** ‚Äî evitando re-execu√ß√£o redundante de todos os agentes:

```python
@router.post("/hitl/{run_id}/approve")
async def approve_review(run_id: str, body: HITLApproveRequest):
    # 1. Carrega revis√£o + conte√∫do do DB
    # 2. Opcionalmente aplica edi√ß√µes
    # 3. Constr√≥i estado LangGraph dos registros do DB
    # 4. Executa synthesizer_node() apenas
    # 5. Atualiza status da revis√£o
    # 6. Retorna an√°lise final
```

---

## Schema do Banco de Dados

Tr√™s tabelas no Supabase PostgreSQL, criadas de forma idempotente na inicializa√ß√£o da aplica√ß√£o:

### `analysis_runs`
| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `run_id` | VARCHAR(36) PK | UUID de cada execu√ß√£o |
| `book` | VARCHAR(50) | Livro b√≠blico |
| `chapter` | INTEGER | N√∫mero do cap√≠tulo |
| `verses` | JSONB | Lista de vers√≠culos |
| `modules` | JSONB | M√≥dulos selecionados |
| `status` | VARCHAR(20) | success / failure / hitl_pending |
| `tokens_consumed` | JSONB | Uso de tokens por n√≥ |
| `model_versions` | JSONB | Nomes de modelo por n√≥ |
| `duration_ms` | INTEGER | Tempo total de execu√ß√£o |
| `error` | TEXT | Mensagem de erro (apenas falhas) |
| `created_at` | TIMESTAMPTZ | Timestamp da execu√ß√£o |

### `analysis_cache`
| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `cache_key` | VARCHAR(64) PK | SHA-256 da entrada |
| `result` | JSONB | Resposta cacheada |
| `hit_count` | INTEGER | Contador at√¥mico de hits |
| `created_at` | TIMESTAMPTZ | Cria√ß√£o da entrada de cache |
| `last_hit_at` | TIMESTAMPTZ | √öltimo acesso |

### `hitl_reviews`
| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `run_id` | VARCHAR(36) PK | Link para analysis_runs |
| `status` | VARCHAR(20) | pending / approved / edited |
| `risk_level` | VARCHAR(10) | high / medium / low |
| `alerts` | JSONB | Alertas do validador |
| `content` | JSONB | Todas as sa√≠das dos agentes |
| `metadata` | JSONB | Vers√µes de modelo, tokens, racioc√≠nio |
| `created_at` | TIMESTAMPTZ | Cria√ß√£o da revis√£o |
| `reviewed_at` | TIMESTAMPTZ | Conclus√£o da revis√£o |

---

## Servi√ßo de Cache

### Gera√ß√£o de Chave

```python
def _build_cache_key(book, chapter, verses, modules):
    raw = f"{book}:{chapter}:{sorted(verses)}:{sorted(modules)}"
    return hashlib.sha256(raw.encode()).hexdigest()
```

### Seguran√ßa contra Race Conditions

Escritas no cache usam `INSERT ... ON CONFLICT DO NOTHING` ‚Äî se duas requisi√ß√µes id√™nticas executam simultaneamente, apenas a primeira escrita √© aceita, e a segunda leitura obt√©m o resultado cacheado.

A contagem de hits usa `UPDATE SET hit_count = hit_count + 1` (opera√ß√£o at√¥mica do PostgreSQL).

---

## Refer√™ncia da API

### POST /analyze

**Requisi√ß√£o:**
```json
{
  "book": "Sl",
  "chapter": 23,
  "verses": [1, 2, 3],
  "selected_modules": ["panorama", "exegese", "teologia"]
}
```

**Resposta (200):**
```json
{
  "final_analysis": "# Estudo Teol√≥gico...\n\n## Panorama...",
  "from_cache": false,
  "run_id": "a1b2c3d4-...",
  "tokens_consumed": {"panorama_agent": {"input": 1200, "output": 3400}},
  "model_versions": {"panorama_agent": "gemini-2.5-flash"},
  "risk_level": "low",
  "hitl_status": null
}
```

**Resposta (200 ‚Äî HITL Pendente):**
```json
{
  "final_analysis": "",
  "hitl_status": "pending",
  "run_id": "a1b2c3d4-...",
  "risk_level": "high"
}
```

### Valida√ß√£o de Input (Pydantic)

- `book`: Sanitizado (strip, title-case)
- `verses`: Deduplicados, ordenados
- `selected_modules`: Deve ser subconjunto de `["panorama", "exegese", "teologia"]`

---

## Deploy

### Dockerfile

```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY requirements-api.txt .
RUN pip install --no-cache-dir -r requirements-api.txt
COPY src/ ./src/
CMD uvicorn main:app --host 0.0.0.0 --port ${PORT} --app-dir src
```

### render.yaml

Blueprint do Render com vari√°veis de ambiente. Secrets (`sync: false`) devem ser configurados manualmente no dashboard:

- `GOOGLE_API_KEY`, `DB_URL`, `LANGSMITH_API_KEY`
- `SMTP_USER`, `SMTP_PASSWORD`, `HITL_REVIEWER_EMAIL`

### Keep-Alive (GitHub Actions)

```yaml
on:
  schedule:
    - cron: '*/14 * * * *'   # A cada 14 minutos
jobs:
  ping:
    runs-on: ubuntu-latest
    steps:
      - run: curl -fsS "${{ secrets.RENDER_API_URL }}/health"
```

**Configura√ß√£o:** Adicione `RENDER_API_URL` como secret em GitHub ‚Üí Settings ‚Üí Secrets ‚Üí Actions.

### Refer√™ncia de Vari√°veis de Ambiente

| Vari√°vel | Obrigat√≥ria | Descri√ß√£o |
|----------|-------------|-----------|
| `GOOGLE_API_KEY` | ‚úÖ | Chave da API Google Gemini |
| `DB_URL` | ‚úÖ | String de conex√£o PostgreSQL Supabase |
| `SMTP_HOST` | ‚úÖ | Servidor SMTP (ex: smtp.gmail.com) |
| `SMTP_PORT` | ‚úÖ | Porta SMTP (587 para TLS) |
| `SMTP_USER` | ‚úÖ | Usu√°rio / email SMTP |
| `SMTP_PASSWORD` | ‚úÖ | App Password do Gmail |
| `HITL_REVIEWER_EMAIL` | ‚úÖ | Email do revisor para alertas HITL |
| `LANGSMITH_API_KEY` | ‚ùå | Tracing via LangSmith |
| `LANGCHAIN_TRACING_V2` | ‚ùå | Ativar tracing (`true`) |
| `LANGCHAIN_PROJECT` | ‚ùå | Nome do projeto no LangSmith |
| `API_BASE_URL` | ‚ùå | Override da URL Streamlit ‚Üí API |
